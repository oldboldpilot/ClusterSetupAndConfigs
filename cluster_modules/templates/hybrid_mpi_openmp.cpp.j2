// Hybrid MPI+OpenMP Benchmark
// Generated by ClusterSetupAndConfigs
// Author: Olumuyiwa Oluwasanmi
// Measures hybrid MPI+OpenMP parallel performance
// C++23 Standard with trailing return type syntax

#include <mpi.h>
#include <omp.h>
#include <iostream>
#include <vector>
#include <chrono>
#include <iomanip>
#include <ranges>
#include <numeric>
#include <cmath>

using namespace std;
using namespace std::chrono;

// Compute-intensive work for hybrid parallelism
auto compute_hybrid_work(size_t work_size, int rank, int thread_id) -> double {
    double result = 0.0;
    size_t offset = (rank * omp_get_max_threads() + thread_id) * 1000;
    
    for (size_t i = 0; i < work_size; ++i) {
        double x = (offset + i) * 0.001;
        result += std::sin(x) * std::cos(x) + std::sqrt(x + 1.0);
    }
    return result;
}

auto measure_hybrid_performance(int num_threads = {{ num_threads }}, 
                               size_t work_size = {{ work_size }}) -> void {
    int rank, nprocs;
    int provided;
    
    MPI_Init_thread(nullptr, nullptr, MPI_THREAD_FUNNELED, &provided);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
    
    // Set number of OpenMP threads
    omp_set_num_threads(num_threads);
    
    if (rank == 0) {
        cout << "Hybrid MPI+OpenMP Performance Benchmark" << endl;
        cout << "========================================" << endl;
        cout << "MPI processes: " << nprocs << endl;
        cout << "OpenMP threads per process: " << num_threads << endl;
        cout << "Total parallelism: " << (nprocs * num_threads) << endl;
        cout << "MPI thread support: ";
        switch(provided) {
            case MPI_THREAD_SINGLE: cout << "SINGLE"; break;
            case MPI_THREAD_FUNNELED: cout << "FUNNELED"; break;
            case MPI_THREAD_SERIALIZED: cout << "SERIALIZED"; break;
            case MPI_THREAD_MULTIPLE: cout << "MULTIPLE"; break;
            default: cout << "UNKNOWN"; break;
        }
        cout << endl;
        cout << "Work size per thread: " << work_size << " iterations" << endl;
        cout << endl;
    }
    
    MPI_Barrier(MPI_COMM_WORLD);
    
    // Run {{ test_iterations }} test iterations
    vector<double> execution_times;
    
    for (int test = 0; test < {{ test_iterations }}; ++test) {
        MPI_Barrier(MPI_COMM_WORLD);
        
        auto start = high_resolution_clock::now();
        
        double local_sum = 0.0;
        
        // Hybrid parallel region: MPI processes + OpenMP threads
        #pragma omp parallel reduction(+:local_sum)
        {
            int tid = omp_get_thread_num();
            double thread_result = compute_hybrid_work(work_size, rank, tid);
            local_sum += thread_result;
        }
        
        // MPI reduction to combine results from all processes
        double global_sum = 0.0;
        MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
        
        auto end = high_resolution_clock::now();
        
        double time_ms = duration_cast<microseconds>(end - start).count() / 1000.0;
        execution_times.push_back(time_ms);
        
        // Prevent optimization
        if (rank == 0) {
            volatile double sink = global_sum;
        }
    }
    
    MPI_Barrier(MPI_COMM_WORLD);
    
    if (rank == 0) {
        // Calculate statistics using C++23 ranges and algorithms
        std::sort(execution_times.begin(), execution_times.end());
        double min_time = *std::min_element(execution_times.begin(), execution_times.end());
        double max_time = *std::max_element(execution_times.begin(), execution_times.end());
        double median_time = execution_times[execution_times.size() / 2];
        double avg_time = std::reduce(execution_times.begin(), execution_times.end(), 0.0) 
                         / execution_times.size();
        
        cout << "Performance Results:" << endl;
        cout << "  Min time:    " << fixed << setprecision(3) << min_time << " ms" << endl;
        cout << "  Max time:    " << max_time << " ms" << endl;
        cout << "  Avg time:    " << avg_time << " ms" << endl;
        cout << "  Median time: " << median_time << " ms" << endl;
        cout << endl;
        
        // Calculate throughput
        size_t total_operations = nprocs * num_threads * work_size;
        double throughput = total_operations / (avg_time / 1000.0);  // ops/sec
        
        cout << "Throughput:" << endl;
        cout << "  Total operations: " << total_operations << endl;
        cout << "  Operations/sec:   " << scientific << setprecision(2) << throughput << endl;
        cout << endl;
    }
    
    // Test MPI communication with OpenMP threads
    if (rank == 0) {
        cout << "Testing MPI Communication Patterns:" << endl;
    }
    
    MPI_Barrier(MPI_COMM_WORLD);
    
    // Point-to-point latency with OpenMP
    if (nprocs >= 2) {
        if (rank == 0 || rank == 1) {
            vector<double> latencies;
            char send_buf[8] = {0};
            char recv_buf[8] = {0};
            
            for (int i = 0; i < 100; ++i) {
                auto start = high_resolution_clock::now();
                
                if (rank == 0) {
                    MPI_Send(send_buf, 8, MPI_CHAR, 1, 0, MPI_COMM_WORLD);
                    MPI_Recv(recv_buf, 8, MPI_CHAR, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                } else {
                    MPI_Recv(recv_buf, 8, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                    MPI_Send(send_buf, 8, MPI_CHAR, 0, 0, MPI_COMM_WORLD);
                }
                
                auto end = high_resolution_clock::now();
                double latency_us = duration_cast<nanoseconds>(end - start).count() / 1000.0;
                latencies.push_back(latency_us);
            }
            
            if (rank == 0) {
                std::sort(latencies.begin(), latencies.end());
                double avg_latency = std::reduce(latencies.begin(), latencies.end(), 0.0) 
                                   / latencies.size();
                cout << "  MPI P2P latency (rank 0 <-> 1): " << fixed << setprecision(3) 
                     << avg_latency << " us" << endl;
            }
        }
    }
    
    MPI_Barrier(MPI_COMM_WORLD);
    
    // Collective operation performance
    if (rank == 0) {
        cout << "  Testing MPI_Allreduce... ";
    }
    
    vector<double> allreduce_times;
    for (int i = 0; i < 100; ++i) {
        double local_val = rank * 1.0;
        double global_val = 0.0;
        
        MPI_Barrier(MPI_COMM_WORLD);
        auto start = high_resolution_clock::now();
        MPI_Allreduce(&local_val, &global_val, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
        auto end = high_resolution_clock::now();
        
        double time_us = duration_cast<nanoseconds>(end - start).count() / 1000.0;
        allreduce_times.push_back(time_us);
    }
    
    if (rank == 0) {
        double avg_allreduce = std::reduce(allreduce_times.begin(), allreduce_times.end(), 0.0) 
                              / allreduce_times.size();
        cout << fixed << setprecision(3) << avg_allreduce << " us" << endl;
        cout << endl;
    }
    
    MPI_Finalize();
}

auto main(int argc, char** argv) -> int {
    int num_threads = {{ num_threads }};
    size_t work_size = {{ work_size }};
    
    if (argc > 1) {
        num_threads = atoi(argv[1]);
    }
    if (argc > 2) {
        work_size = atoi(argv[2]);
    }
    
    measure_hybrid_performance(num_threads, work_size);
    
    return 0;
}
