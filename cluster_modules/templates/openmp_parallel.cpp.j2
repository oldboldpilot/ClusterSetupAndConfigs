// OpenMP Parallel Performance Benchmark
// Generated by ClusterSetupAndConfigs
// Author: Olumuyiwa Oluwasanmi
// Measures OpenMP thread-level parallelism performance
// C++23 Standard with trailing return type syntax

#include <omp.h>
#include <iostream>
#include <vector>
#include <chrono>
#include <iomanip>
#include <algorithm>
#include <ranges>
#include <numeric>
#include <cmath>

using namespace std;
using namespace std::chrono;

// Compute-intensive function for benchmarking
auto compute_work(size_t iterations) -> double {
    double result = 0.0;
    for (size_t i = 0; i < iterations; ++i) {
        result += std::sin(i * 0.001) * std::cos(i * 0.001);
    }
    return result;
}

auto measure_openmp_performance(int num_threads = {{ num_threads }}, size_t work_size = {{ work_size }}) -> void {
    cout << "OpenMP Parallel Performance Benchmark" << endl;
    cout << "======================================" << endl;
    cout << "Maximum threads available: " << omp_get_max_threads() << endl;
    cout << "Work size per thread: " << work_size << " iterations" << endl;
    cout << endl;
    
    // Test different thread counts
    vector<int> thread_counts;
    for (int t = 1; t <= num_threads; t *= 2) {
        thread_counts.push_back(t);
    }
    if (thread_counts.back() != num_threads) {
        thread_counts.push_back(num_threads);
    }
    
    for (int nthreads : thread_counts) {
        omp_set_num_threads(nthreads);
        
        vector<double> execution_times;
        
        // Run {{ test_iterations }} tests
        for (int test = 0; test < {{ test_iterations }}; ++test) {
            auto start = high_resolution_clock::now();
            
            #pragma omp parallel
            {
                double local_result = compute_work(work_size);
                // Prevent optimization from removing computation
                volatile double sink = local_result;
            }
            
            auto end = high_resolution_clock::now();
            double time_ms = duration_cast<microseconds>(end - start).count() / 1000.0;
            execution_times.push_back(time_ms);
        }
        
        // Calculate statistics using C++23 ranges and algorithms
        std::sort(execution_times.begin(), execution_times.end());
        double min_time = *std::min_element(execution_times.begin(), execution_times.end());
        double max_time = *std::max_element(execution_times.begin(), execution_times.end());
        double median_time = execution_times[execution_times.size() / 2];
        double avg_time = std::reduce(execution_times.begin(), execution_times.end(), 0.0) / execution_times.size();
        
        // Calculate speedup relative to single thread
        static double baseline_time = 0.0;
        if (nthreads == 1) {
            baseline_time = avg_time;
        }
        double speedup = (baseline_time > 0) ? baseline_time / avg_time : 1.0;
        double efficiency = speedup / nthreads * 100.0;
        
        cout << "Threads: " << nthreads << endl;
        cout << "  Min time:    " << fixed << setprecision(3) << min_time << " ms" << endl;
        cout << "  Max time:    " << max_time << " ms" << endl;
        cout << "  Avg time:    " << avg_time << " ms" << endl;
        cout << "  Median time: " << median_time << " ms" << endl;
        if (nthreads > 1) {
            cout << "  Speedup:     " << setprecision(2) << speedup << "x" << endl;
            cout << "  Efficiency:  " << setprecision(1) << efficiency << "%" << endl;
        }
        cout << endl;
    }
}

auto main(int argc, char** argv) -> int {
    int num_threads = {{ num_threads }};
    size_t work_size = {{ work_size }};
    
    if (argc > 1) {
        num_threads = atoi(argv[1]);
    }
    if (argc > 2) {
        work_size = atoi(argv[2]);
    }
    
    measure_openmp_performance(num_threads, work_size);
    
    return 0;
}
