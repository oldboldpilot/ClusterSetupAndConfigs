// MPI Point-to-Point Latency Benchmark
// Generated by ClusterSetupAndConfigs
// Author: Olumuyiwa Oluwasanmi
// Measures round-trip latency using MPI_Send/MPI_Recv

#include <mpi.h>
#include <iostream>
#include <vector>
#include <algorithm>
#include <chrono>
#include <iomanip>

using namespace std;
using namespace std::chrono;

void measure_latency(int iterations = {{ iterations }}) {
    int rank, nprocs;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
    
    if (nprocs < 2) {
        if (rank == 0) {
            cerr << "Error: This benchmark requires at least 2 processes" << endl;
        }
        return;
    }
    
    // Synchronize all processes
    MPI_Barrier(MPI_COMM_WORLD);
    
    if (rank == 0) {
        cout << "MPI Point-to-Point Latency Benchmark" << endl;
        cout << "=====================================" << endl;
        cout << "Number of processes: " << nprocs << endl;
        cout << "Iterations per measurement: " << iterations << endl;
        cout << "Message size: {{ message_size }} bytes" << endl;
        cout << endl;
        
        char send_buf[{{ message_size }}] = {0};
        char recv_buf[{{ message_size }}] = {0};
        
        // Measure latency to each other rank
        for (int target = 1; target < nprocs; target++) {
            vector<double> latencies;
            
            // Warmup
            for (int i = 0; i < {{ warmup_iterations }}; i++) {
                MPI_Send(send_buf, {{ message_size }}, MPI_CHAR, target, 0, MPI_COMM_WORLD);
                MPI_Recv(recv_buf, {{ message_size }}, MPI_CHAR, target, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            }
            
            // Actual measurements
            for (int i = 0; i < iterations; i++) {
                auto start = high_resolution_clock::now();
                MPI_Send(send_buf, {{ message_size }}, MPI_CHAR, target, 0, MPI_COMM_WORLD);
                MPI_Recv(recv_buf, {{ message_size }}, MPI_CHAR, target, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                auto end = high_resolution_clock::now();
                
                double latency_us = duration_cast<nanoseconds>(end - start).count() / 1000.0;
                latencies.push_back(latency_us);
            }
            
            // Calculate statistics
            sort(latencies.begin(), latencies.end());
            double min_lat = latencies.front();
            double max_lat = latencies.back();
            double median_lat = latencies[latencies.size() / 2];
            double avg_lat = 0.0;
            for (double lat : latencies) avg_lat += lat;
            avg_lat /= latencies.size();
            
            cout << "Rank 0 <-> Rank " << target << ":" << endl;
            cout << "  Min latency:    " << fixed << setprecision(3) << min_lat << " us" << endl;
            cout << "  Max latency:    " << max_lat << " us" << endl;
            cout << "  Avg latency:    " << avg_lat << " us" << endl;
            cout << "  Median latency: " << median_lat << " us" << endl;
            cout << endl;
        }
    } else {
        // Worker ranks echo messages back
        char send_buf[{{ message_size }}] = {0};
        char recv_buf[{{ message_size }}] = {0};
        
        MPI_Barrier(MPI_COMM_WORLD);
        
        // Warmup + actual measurements
        int total_iterations = {{ warmup_iterations }} + {{ iterations }};
        for (int i = 0; i < total_iterations; i++) {
            MPI_Recv(recv_buf, {{ message_size }}, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            MPI_Send(send_buf, {{ message_size }}, MPI_CHAR, 0, 0, MPI_COMM_WORLD);
        }
    }
}

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);
    
    int iterations = {{ iterations }};
    if (argc > 1) {
        iterations = atoi(argv[1]);
    }
    
    measure_latency(iterations);
    
    MPI_Finalize();
    return 0;
}
